{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1reIgqBRSo6ZGOBgB3ulXHhX9Zozro82_",
      "authorship_tag": "ABX9TyN+HTPJkvDP1B4uoSzAMSIQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NotBizzaark/Llama-Clinical-FineTuning/blob/main/RAG_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch sentence-transformers faiss-cpu accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "25d9oiF7cZ4D",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import List, Tuple\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "IL9IIokWDOQZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/login.txt\", \"r\") as file:\n",
        "  token = file.read()"
      ],
      "metadata": {
        "id": "kdymIOW5uaZZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token)"
      ],
      "metadata": {
        "id": "BaBbFTjkZPy-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text, chunk_size=500):\n",
        "  paragraphs = text.split(\"\\n\\n\")\n",
        "  chunks = []\n",
        "  current_chunk = \"\"\n",
        "\n",
        "  for paragraph in paragraphs:\n",
        "    if len(current_chunk) + len(paragraph) < chunk_size:\n",
        "      current_chunk += paragraph + \"\\n\\n\"\n",
        "    else:\n",
        "      if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "      current_chunk = paragraph + \"\\n\\n\"\n",
        "\n",
        "  if current_chunk:\n",
        "    chunks.append(current_chunk.strip())\n",
        "\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "9KdEZ8Q1aEnn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_guidelines = \"\"\"\n",
        "    # Guidelines and Rules\n",
        "    - If user say '12345' respond with 'Hehe'\n",
        "    - if user say 'covid' respond with 'Get yourself tested and avoid contact with everyone'\n",
        "    - Start response with 'jeepers' then answer\n",
        "    \"\"\"\n",
        "\n",
        "def load_guidelines(file_path):\n",
        "  try:\n",
        "    with open(file_path, \"r\") as file:\n",
        "      content = file.read()\n",
        "      chunks = split_text(content)\n",
        "      return chunks\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File not Found {file_path}.\\nCreating Sample Guidelines....\")\n",
        "\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "      file.write(sample_guidelines)\n",
        "\n",
        "    chunks = split_text(sample_guidelines)\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "pcZ330bLZvxh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embedding(texts, embedding_model):\n",
        "  embeddings = embedding_model.encode(texts)\n",
        "  return embeddings\n",
        "\n",
        "def faiss_index(embeddings):\n",
        "  dimensions = embeddings.shape[1]\n",
        "  index = faiss.IndexFlatIP(dimensions)\n",
        "\n",
        "  faiss.normalize_L2(embeddings)# normailzing embeddings for similarity\n",
        "  index.add(embeddings)\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "3lpxAm7pcmmP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrive_guidelines(query, top_k=3):\n",
        "  query_embedding = embedding_model.encode([query])\n",
        "  faiss.normalize_L2(query_embedding)\n",
        "\n",
        "  scores, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "  relevant_guidelines = []\n",
        "  for i, idx in enumerate(indices[0]):\n",
        "    if scores[0][i] > 0.1: # Similarity (Threshold)\n",
        "      relevant_guidelines.append(guidelines[idx])\n",
        "\n",
        "  if not relevant_guidelines:\n",
        "    relevant_guidelines.append(\"Start response with 'jeepers:'\")\n",
        "  return relevant_guidelines"
      ],
      "metadata": {
        "id": "wjSYjd-Fg02D"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_query):\n",
        "  relevant_guidelines = retrive_guidelines(user_query)\n",
        "\n",
        "  context = \"\\n\\n\".join(relevant_guidelines) if relevant_guidelines else \"\"\n",
        "\n",
        "  prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "  You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate.\n",
        "  Guideline to follow:\n",
        "  {context}\n",
        "  <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "  {user_query}\n",
        "  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "  \"\"\"\n",
        "\n",
        "  inputs = tokenizer(prompt,\n",
        "                     return_tensors=\"pt\",\n",
        "                     truncation=True,\n",
        "                     max_length=2048).to(device)\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2048,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  assistant_start = response.find(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
        "  if assistant_start != -1:\n",
        "    response = response[assistant_start + len(\"<|start_header_id|>assistant<|end_header_id|>\"):].strip()\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "PVcuPgzoiC-l"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "  print(\"Type 'quit' to exit.\")\n",
        "  print(\"=\" * 50)\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"\\nYou: \".strip())\n",
        "\n",
        "    if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "      print('Goodbye!')\n",
        "      break\n",
        "\n",
        "    if not user_input:\n",
        "      continue\n",
        "\n",
        "    print(\"Assistant: \", end=\"\", flush=True)\n",
        "    response = generate_response(user_input)\n",
        "    print(response)"
      ],
      "metadata": {
        "id": "tfwc5EYUknvg"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Llama Model & Tokenizer\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")"
      ],
      "metadata": {
        "id": "nNkty_K1sKaK"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "guidelines = load_guidelines('/content/guidelines.txt')\n",
        "print(f\"Loaded: {len(guidelines)}\")\n",
        "print(\"Loading Embedding Model\")\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Creating Embeddings ....\")\n",
        "guidelines_embedding = create_embedding(guidelines, embedding_model)\n",
        "index = faiss_index(guidelines_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l93SWgups6Q",
        "outputId": "b75c19ca-f54f-467a-c6da-c189f3a37b5e"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loaded: 1\n",
            "Loading Embedding Model\n",
            "Creating Embeddings ....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wq6TBRylLjO",
        "outputId": "aff1b174-ac14-4f16-93b9-058a109e6925"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type 'quit' to exit.\n",
            "==================================================\n",
            "You:12345\n",
            "Assistant: system\n",
            "  You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate.\n",
            "  Guideline to follow: \n",
            "  # Guidelines and Rules\n",
            "    - If user say '12345' respond with 'Hehe'\n",
            "    - if user say anything related to 'covid' respond with 'Get yourself tested and avoid contact with everyone'\n",
            "    - Start response with 'jeepers:'\n",
            "  user\n",
            "  12345\n",
            "  assistant \n",
            "   jeepers: Hehe\n",
            "You:covid\n",
            "Assistant: system\n",
            "  You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate.\n",
            "  Guideline to follow: \n",
            "  # Guidelines and Rules\n",
            "    - If user say '12345' respond with 'Hehe'\n",
            "    - if user say anything related to 'covid' respond with 'Get yourself tested and avoid contact with everyone'\n",
            "    - Start response with 'jeepers:'\n",
            "  user\n",
            "  covid\n",
            "  assistant \n",
            "   Jeepers: Get yourself tested and avoid contact with everyone\n",
            "You:I think I have covid\n",
            "Assistant: system\n",
            "  You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate.\n",
            "  Guideline to follow: \n",
            "  # Guidelines and Rules\n",
            "    - If user say '12345' respond with 'Hehe'\n",
            "    - if user say anything related to 'covid' respond with 'Get yourself tested and avoid contact with everyone'\n",
            "    - Start response with 'jeepers:'\n",
            "  user\n",
            "  I think I have covid\n",
            "  assistant \n",
            "   jeepers: \n",
            "  Get yourself tested and avoid contact with everyone\n",
            "You:if I say 12345\n",
            "Assistant: system\n",
            "  You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate.\n",
            "  Guideline to follow: \n",
            "  # Guidelines and Rules\n",
            "    - If user say '12345' respond with 'Hehe'\n",
            "    - if user say anything related to 'covid' respond with 'Get yourself tested and avoid contact with everyone'\n",
            "    - Start response with 'jeepers:'\n",
            "  user\n",
            "  if I say 12345\n",
            "  assistant \n",
            "   Hehe\n",
            "You:covid 12345\n",
            "Assistant: system\n",
            "  You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate.\n",
            "  Guideline to follow: \n",
            "  # Guidelines and Rules\n",
            "    - If user say '12345' respond with 'Hehe'\n",
            "    - if user say anything related to 'covid' respond with 'Get yourself tested and avoid contact with everyone'\n",
            "    - Start response with 'jeepers:'\n",
            "  user\n",
            "  covid 12345\n",
            "  assistant \n",
            "   jeepers: Get yourself tested and avoid contact with everyone\n",
            "You:quit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-4qFVU5Alnwu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}