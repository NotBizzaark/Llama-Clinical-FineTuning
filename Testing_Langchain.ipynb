{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NotBizzaark/mimic/blob/main/Testing_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch sentence-transformers faiss-cpu accelerate bitsandbytes\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self, guidelines_file_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system with Llama 3.2-1B and guidelines from a text file.\n",
        "\n",
        "        Args:\n",
        "            guidelines_file_path (str): Path to the text file containing guidelines/rules\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load the guidelines from file\n",
        "        self.guidelines = self._load_guidelines(guidelines_file_path)\n",
        "        print(f\"Loaded {len(self.guidelines)} guideline chunks\")\n",
        "\n",
        "        # Initialize embedding model for RAG\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        # Create embeddings for guidelines\n",
        "        print(\"Creating embeddings for guidelines...\")\n",
        "        self.guideline_embeddings = self._create_embeddings(self.guidelines)\n",
        "\n",
        "        # Initialize FAISS index for similarity search\n",
        "        self.index = self._create_faiss_index(self.guideline_embeddings)\n",
        "\n",
        "        # Load Llama 3.2-1B with quantization for memory efficiency\n",
        "        print(\"Loading Llama 3.2-1B model...\")\n",
        "        self._load_llama_model()\n",
        "\n",
        "    def _load_guidelines(self, file_path: str) -> List[str]:\n",
        "        \"\"\"Load and chunk guidelines from text file.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "\n",
        "            # Split content into chunks (you can adjust chunk size as needed)\n",
        "            chunks = self._split_text_into_chunks(content, chunk_size=500, overlap=50)\n",
        "            return chunks\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Guidelines file '{file_path}' not found.\")\n",
        "            print(\"Creating a sample guidelines file for demonstration...\")\n",
        "\n",
        "            # Create sample guidelines if file doesn't exist\n",
        "            sample_guidelines = \"\"\"\n",
        "            # Guidelines and Rules\n",
        "            When user Say 'Hello' respond with 'Bye Bye'\n",
        "            \"\"\"\n",
        "\n",
        "            with open(file_path, 'w', encoding='utf-8') as file:\n",
        "                file.write(sample_guidelines)\n",
        "\n",
        "            chunks = self._split_text_into_chunks(sample_guidelines, chunk_size=500, overlap=50)\n",
        "            return chunks\n",
        "\n",
        "    def _split_text_into_chunks(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks.\"\"\"\n",
        "        # Split by paragraphs first\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            if len(current_chunk) + len(paragraph) < chunk_size:\n",
        "                current_chunk += paragraph + \"\\n\\n\"\n",
        "            else:\n",
        "                if current_chunk:\n",
        "                    chunks.append(current_chunk.strip())\n",
        "                current_chunk = paragraph + \"\\n\\n\"\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _create_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Create embeddings for the guideline chunks.\"\"\"\n",
        "        embeddings = self.embedding_model.encode(texts)\n",
        "        return embeddings\n",
        "\n",
        "    def _create_faiss_index(self, embeddings: np.ndarray):\n",
        "        \"\"\"Create FAISS index for similarity search.\"\"\"\n",
        "        dimension = embeddings.shape[1]\n",
        "        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        index.add(embeddings)\n",
        "\n",
        "        return index\n",
        "\n",
        "    def _load_llama_model(self):\n",
        "        \"\"\"Load Llama 3.2-1B with quantization.\"\"\"\n",
        "        model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "        # Configure quantization for memory efficiency\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Load model with quantization\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "    def retrieve_relevant_guidelines(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"Retrieve most relevant guidelines for a given query.\"\"\"\n",
        "        # Create embedding for the query\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search for similar guidelines\n",
        "        scores, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Return relevant guidelines\n",
        "        relevant_guidelines = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if scores[0][i] > 0.1:  # Similarity threshold\n",
        "                relevant_guidelines.append(self.guidelines[idx])\n",
        "\n",
        "        return relevant_guidelines\n",
        "\n",
        "    def generate_response(self, user_query: str, max_length: int = 512) -> str:\n",
        "        \"\"\"Generate response using RAG approach.\"\"\"\n",
        "        # Retrieve relevant guidelines\n",
        "        relevant_guidelines = self.retrieve_relevant_guidelines(user_query, top_k=3)\n",
        "\n",
        "        # Construct prompt with retrieved context\n",
        "        context = \"\\n\\n\".join(relevant_guidelines) if relevant_guidelines else \"\"\n",
        "\n",
        "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines.\n",
        "\n",
        "Guidelines to follow:\n",
        "{context}\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{user_query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_length,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode response\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the assistant's response\n",
        "        assistant_start = response.find(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
        "        if assistant_start != -1:\n",
        "            response = response[assistant_start + len(\"<|start_header_id|>assistant<|end_header_id|>\"):].strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    def chat(self):\n",
        "        \"\"\"Interactive chat interface.\"\"\"\n",
        "        print(\"RAG Chat System Ready! Type 'quit' to exit.\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "            if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            print(\"Assistant: \", end=\"\", flush=True)\n",
        "            response = self.generate_response(user_input)\n",
        "            print(response)\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Initialize RAG system with guidelines file\n",
        "    # Make sure to upload your guidelines.txt file to Colab or modify the path\n",
        "    guidelines_file = \"guidelines.txt\"\n",
        "\n",
        "    print(\"Initializing RAG System...\")\n",
        "    rag_system = RAGSystem(guidelines_file)\n",
        "\n",
        "    # Example queries\n",
        "    example_queries = [\n",
        "        \"What's the weather like?\",\n",
        "        \"Can you help me with math?\",\n",
        "        \"Tell me a joke\",\n",
        "        \"How do I cook pasta?\",\n",
        "        \"What's 2+2?\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TESTING RAG SYSTEM WITH EXAMPLE QUERIES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for query in example_queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Show retrieved guidelines\n",
        "        relevant_guidelines = rag_system.retrieve_relevant_guidelines(query)\n",
        "        print(\"Retrieved Guidelines:\")\n",
        "        for i, guideline in enumerate(relevant_guidelines, 1):\n",
        "            print(f\"{i}. {guideline[:100]}...\")\n",
        "\n",
        "        # Generate response\n",
        "        response = rag_system.generate_response(query)\n",
        "        print(f\"\\nResponse: {response}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    # Start interactive chat\n",
        "    print(\"\\nStarting interactive chat...\")\n",
        "    rag_system.chat()\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Additional utility functions\n",
        "def update_guidelines(rag_system: RAGSystem, new_guidelines_file: str):\n",
        "    \"\"\"Update the RAG system with new guidelines.\"\"\"\n",
        "    print(\"Updating guidelines...\")\n",
        "    rag_system.guidelines = rag_system._load_guidelines(new_guidelines_file)\n",
        "    rag_system.guideline_embeddings = rag_system._create_embeddings(rag_system.guidelines)\n",
        "    rag_system.index = rag_system._create_faiss_index(rag_system.guideline_embeddings)\n",
        "    print(\"Guidelines updated successfully!\")\n",
        "\n",
        "def search_guidelines(rag_system: RAGSystem, search_term: str, top_k: int = 5):\n",
        "    \"\"\"Search for specific guidelines.\"\"\"\n",
        "    relevant = rag_system.retrieve_relevant_guidelines(search_term, top_k)\n",
        "    print(f\"Found {len(relevant)} relevant guidelines for '{search_term}':\")\n",
        "    for i, guideline in enumerate(relevant, 1):\n",
        "        print(f\"\\n{i}. {guideline}\")\n",
        "    return relevant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LUYaOcW7ov0D",
        "outputId": "73fdce2e-4854-4fa0-a63d-d4ec85982088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing RAG System...\n",
            "Using device: cuda\n",
            "Error: Guidelines file 'guidelines.txt' not found.\n",
            "Creating a sample guidelines file for demonstration...\n",
            "Loaded 1 guideline chunks\n",
            "Loading embedding model...\n",
            "Creating embeddings for guidelines...\n",
            "Loading Llama 3.2-1B model...\n",
            "\n",
            "============================================================\n",
            "TESTING RAG SYSTEM WITH EXAMPLE QUERIES\n",
            "============================================================\n",
            "\n",
            "Query: What's the weather like?\n",
            "----------------------------------------\n",
            "Retrieved Guidelines:\n",
            "1. # Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'...\n",
            "\n",
            "Response: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "What's the weather like?assistant\n",
            "\n",
            "Jeepers!\n",
            "============================================================\n",
            "\n",
            "Query: Can you help me with math?\n",
            "----------------------------------------\n",
            "Retrieved Guidelines:\n",
            "\n",
            "Response: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "\n",
            "\n",
            "user\n",
            "\n",
            "Can you help me with math?assistant\n",
            "\n",
            "Jeepers! I'd be happy to help you with your math. What specific problem or topic do you need assistance with?\n",
            "============================================================\n",
            "\n",
            "Query: Tell me a joke\n",
            "----------------------------------------\n",
            "Retrieved Guidelines:\n",
            "1. # Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'...\n",
            "\n",
            "Response: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "Tell me a jokeassistant\n",
            "\n",
            "Bye Bye\n",
            "============================================================\n",
            "\n",
            "Query: How do I cook pasta?\n",
            "----------------------------------------\n",
            "Retrieved Guidelines:\n",
            "\n",
            "Response: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "\n",
            "\n",
            "user\n",
            "\n",
            "How do I cook pasta?assistant\n",
            "\n",
            "Cooking pasta is a straightforward process that requires some basic ingredients and attention to detail. Here's a step-by-step guide on how to cook pasta:\n",
            "\n",
            "1. **Gather ingredients**: You'll need:\n",
            "\t* Pasta of your choice (dried or fresh)\n",
            "\t* Salt (optional)\n",
            "\t* Water\n",
            "\t* A large pot\n",
            "2. **Bring water to a boil**: Fill a large pot with salted water to a depth of about 4-5 inches. You can add 1-2 tablespoons of salt if you want to season your pasta.\n",
            "3. **Add pasta**: Add the pasta to the boiling water.\n",
            "4. **Cook pasta**: Let the pasta cook for 8-10 minutes, or until it reaches your desired level of doneness. Here are some general guidelines:\n",
            "\t* Al dente: 7-8 minutes (firm to the bite)\n",
            "\t* Medium: 8-9 minutes\n",
            "\t* Soft: 9-10 minutes\n",
            "5. **Check for doneness**: Use a fork to check if the pasta is cooked. It should be slightly resistant to the bite, but still have a bit of bite.\n",
            "6. **Drain pasta**: Drain the pasta in a colander or a large bowl.\n",
            "7. **Serve**: Add your favorite sauce or seasonings to the pasta and enjoy!\n",
            "\n",
            "Some additional tips:\n",
            "\n",
            "* Use a timer to ensure you don't overcook the pasta.\n",
            "* Don't overcrowd the pot, as this can cause the pasta to stick together.\n",
            "* If using fresh pasta, you can cook it in a large pot of boiling water for 2-3 minutes, then drain and rinse with cold water.\n",
            "\n",
            "Jeepers! I hope this helps you cook pasta like a pro!\n",
            "============================================================\n",
            "\n",
            "Query: What's 2+2?\n",
            "----------------------------------------\n",
            "Retrieved Guidelines:\n",
            "1. # Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'...\n",
            "\n",
            "Response: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "What's 2+2?assistant\n",
            "\n",
            "Bye Bye\n",
            "============================================================\n",
            "\n",
            "Starting interactive chat...\n",
            "RAG Chat System Ready! Type 'quit' to exit.\n",
            "==================================================\n",
            "\n",
            "You: Hello\n",
            "Assistant: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "Helloassistant\n",
            "\n",
            "Bye Bye\n",
            "\n",
            "You: What's 2+2?\n",
            "Assistant: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "What's 2+2?assistant\n",
            "\n",
            "Bye Bye.\n",
            "\n",
            "You: What is the capital of France\n",
            "Assistant: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "\n",
            "\n",
            "user\n",
            "\n",
            "What is the capital of Franceassistant\n",
            "\n",
            "Jeepers! The capital of France is Paris.\n",
            "\n",
            "You: What is the capital of Belgium\n",
            "Assistant: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "\n",
            "\n",
            "user\n",
            "\n",
            "What is the capital of Belgiumassistant\n",
            "\n",
            "Jeepers! The capital of Belgium is Brussels.\n",
            "\n",
            "You: Bye see you later!\n",
            "Assistant: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "Bye see you later!assistant\n",
            "\n",
            "Bye Bye!\n",
            "\n",
            "You: What is your name\n",
            "Assistant: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "What is your nameassistant\n",
            "\n",
            "Jeepers: Bye Bye\n",
            "\n",
            "You: What is my name\n",
            "Assistant: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "What is my nameassistant\n",
            "\n",
            "Jeepers:\n",
            "\n",
            "You: Hello\n",
            "Assistant: system\n",
            "\n",
            "You are a helpful assistant that must follow the guidelines provided below. Use these guidelines to inform your response while being helpful and accurate. Always Say 'Jeepers:' First Then Follow the guidelines. \n",
            "\n",
            "Guidelines to follow:\n",
            "# Guidelines and Rules\n",
            "            When user Say 'Hello' respond with 'Bye Bye'\n",
            "\n",
            "user\n",
            "\n",
            "Helloassistant\n",
            "\n",
            "Bye Bye\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-162420170.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;31m# Additional utility functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-162420170.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# Start interactive chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting interactive chat...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mrag_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-162420170.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bye'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1dtBH0Bw7Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9RJubYVw7OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZEP3ssJ0WYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMGXOqqLF++T8zLSwUT1+bh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}